{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560d1379-9c97-44eb-b7d1-0a777a90d52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:16:30 WARN Utils: Your hostname, vbox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/17 20:16:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 20:16:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ETL_Amazon_Electronics_2023\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", str(128 * 1024 * 1024))  # <-- en string\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Rutas HDFS\n",
    "landing_path = \"hdfs://localhost:9000/datalake/landing/Electronics.jsonl\"\n",
    "bronze_2023  = \"hdfs://localhost:9000/datalake/bronze/amazon/electronics/reviews/curated_2023/\"\n",
    "silver_2023  = \"hdfs://localhost:9000/datalake/silver/amazon/electronics/reviews_clean_2023/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18533345-443a-4d2a-bf3c-498a19e6ca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(167 + 2) / 169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw rows: 43886944\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "landing_path = \"hdfs://localhost:9000/datalake/landing/Electronics.jsonl\"  # o .jsonl\n",
    "bronze_2023_path = \"hdfs://localhost:9000/datalake/bronze/amazon/electronics/reviews/curated_2023/\"\n",
    "\n",
    "# --- Schema explícito (incluye _corrupt_record) ---\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"asin\", T.StringType(), True),\n",
    "    T.StructField(\"helpful_vote\", T.LongType(), True),\n",
    "    T.StructField(\"images\", T.ArrayType(T.StructType([\n",
    "        T.StructField(\"large_image_url\",  T.StringType(), True),\n",
    "        T.StructField(\"medium_image_url\", T.StringType(), True),\n",
    "        T.StructField(\"small_image_url\",  T.StringType(), True),\n",
    "        T.StructField(\"attachment_type\",  T.StringType(), True),\n",
    "    ])), True),\n",
    "    T.StructField(\"parent_asin\", T.StringType(), True),\n",
    "    T.StructField(\"rating\", T.DoubleType(), True),\n",
    "    T.StructField(\"text\", T.StringType(), True),\n",
    "    T.StructField(\"timestamp\", T.LongType(), True),     # s o ms\n",
    "    T.StructField(\"title\", T.StringType(), True),\n",
    "    T.StructField(\"user_id\", T.StringType(), True),\n",
    "    T.StructField(\"verified_purchase\", T.BooleanType(), True),\n",
    "    T.StructField(\"_corrupt_record\", T.StringType(), True),  # <-- clave para filtrar\n",
    "])\n",
    "\n",
    "df_raw = (\n",
    "    spark.read\n",
    "      .option(\"mode\", \"PERMISSIVE\")\n",
    "      .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "      .schema(schema)\n",
    "      .json(landing_path)\n",
    ")\n",
    "print(\"raw rows:\", df_raw.count())\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7f98bf-073f-4784-9170-6300840b7a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze 2023 rows: 1912874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|min_fecha          |max_fecha          |\n",
      "+-------------------+-------------------+\n",
      "|2022-12-31 19:00:06|2023-09-13 12:26:21|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Límites de 2023 en epoch segundos (UTC)\n",
    "ts_min = 1672531200   # 2023-01-01 00:00:00\n",
    "ts_max = 1704067199   # 2023-12-31 23:59:59\n",
    "epoch_ms_threshold = F.lit(10**12)  # si timestamp > 10^12, asumimos milisegundos\n",
    "\n",
    "# Normalizar a segundos\n",
    "ts_sec = F.when(F.col(\"timestamp\").isNotNull() & (F.col(\"timestamp\") > epoch_ms_threshold),\n",
    "                (F.col(\"timestamp\")/1000).cast(\"long\")\n",
    "         ).otherwise(F.col(\"timestamp\").cast(\"long\"))\n",
    "\n",
    "df_bronze_2023 = (\n",
    "    df_raw\n",
    "      .filter(F.col(\"_corrupt_record\").isNull())         # <--- evita filas dañadas\n",
    "      .withColumn(\"ts_sec\", ts_sec)\n",
    "      .filter(F.col(\"ts_sec\").between(ts_min, ts_max))   # <--- filtro 2023\n",
    "      .withColumn(\"ts_review\", F.to_timestamp(F.from_unixtime(\"ts_sec\")))\n",
    "      .withColumn(\"dt\", F.date_format(\"ts_review\", \"yyyy-MM\"))  # partición\n",
    "      .withColumn(\"asin\", F.upper(\"asin\"))\n",
    ")\n",
    "\n",
    "print(\"bronze 2023 rows:\", df_bronze_2023.count())\n",
    "df_bronze_2023.select(\n",
    "    F.min(\"ts_review\").alias(\"min_fecha\"),\n",
    "    F.max(\"ts_review\").alias(\"max_fecha\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d2f4a3-8c8b-4b75-9ef2-e6aa737094df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escrito en: hdfs://localhost:9000/datalake/bronze/amazon/electronics/reviews/curated_2023/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "(df_bronze_2023\n",
    "   .repartition(\"dt\")\n",
    "   .write.mode(\"overwrite\")\n",
    "   .partitionBy(\"dt\")\n",
    "   .parquet(bronze_2023_path)\n",
    ")\n",
    "\n",
    "print(\"Escrito en:\", bronze_2023_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c0323f-bf81-46e8-a3f0-4e90251eebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: 1912874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|dt     |count |\n",
      "+-------+------+\n",
      "|2022-12|2897  |\n",
      "|2023-01|469002|\n",
      "|2023-02|377300|\n",
      "|2023-03|423348|\n",
      "|2023-04|255710|\n",
      "|2023-05|147819|\n",
      "|2023-06|95917 |\n",
      "|2023-07|80960 |\n",
      "|2023-08|56344 |\n",
      "|2023-09|3577  |\n",
      "+-------+------+\n",
      "\n",
      "+-------------------+-------------------+\n",
      "|min_ts             |max_ts             |\n",
      "+-------------------+-------------------+\n",
      "|2022-12-31 19:00:06|2023-09-13 12:26:21|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_b = spark.read.parquet(bronze_2023_path)\n",
    "print(\"files:\", df_b.count())\n",
    "df_b.select(\"dt\").groupBy(\"dt\").count().orderBy(\"dt\").show(12, False)\n",
    "df_b.select(F.min(\"ts_review\").alias(\"min_ts\"),\n",
    "            F.max(\"ts_review\").alias(\"max_ts\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8420055d-4f41-455b-9948-8dbbef2d6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver Clean escrito en: hdfs://localhost:9000/datalake/silver/amazon/electronics/reviews_clean_2023/\n",
      "Filas Silver: 1884789\n",
      "+-------+------+\n",
      "|dt     |count |\n",
      "+-------+------+\n",
      "|2022-12|2865  |\n",
      "|2023-01|464076|\n",
      "|2023-02|373165|\n",
      "|2023-03|418434|\n",
      "|2023-04|251695|\n",
      "|2023-05|144514|\n",
      "|2023-06|93516 |\n",
      "|2023-07|78673 |\n",
      "|2023-08|54422 |\n",
      "|2023-09|3429  |\n",
      "+-------+------+\n",
      "\n",
      "+-------------------+-------------------+\n",
      "|min_fecha          |max_fecha          |\n",
      "+-------------------+-------------------+\n",
      "|2022-12-31 19:00:06|2023-09-13 12:26:21|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window as W\n",
    "\n",
    "# Paths\n",
    "bronze_2023_path = \"hdfs://localhost:9000/datalake/bronze/amazon/electronics/reviews/curated_2023/\"\n",
    "silver_2023_path = \"hdfs://localhost:9000/datalake/silver/amazon/electronics/reviews_clean_2023/\"\n",
    "\n",
    "# 1) Leer Bronze y E S T A N D A R I Z A R nombres\n",
    "df_b = spark.read.parquet(bronze_2023_path)\n",
    "\n",
    "# Mapea nombres Bronze -> estándar\n",
    "df_std = (\n",
    "    df_b\n",
    "    .select(\n",
    "        F.col(\"asin\"),\n",
    "        F.col(\"text\").alias(\"reviewText\"),          # <-- text -> reviewText\n",
    "        F.col(\"title\"),\n",
    "        F.col(\"rating\").alias(\"overall\"),           # <-- rating -> overall\n",
    "        F.col(\"helpful_vote\").alias(\"helpful_votes\"),\n",
    "        F.col(\"user_id\"),\n",
    "        F.col(\"verified_purchase\"),\n",
    "        F.col(\"ts_review\"),\n",
    "        F.col(\"dt\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Limpieza de texto (sin UDF)\n",
    "txt = F.col(\"reviewText\").cast(\"string\")\n",
    "txt_clean = F.lower(txt)\n",
    "txt_clean = F.regexp_replace(txt_clean, r\"http[s]?://\\S+|www\\.\\S+\", \"\")      # URLs\n",
    "txt_clean = F.regexp_replace(txt_clean, r\"[\\p{C}\\p{So}\\p{Sk}]+\", \" \")        # control/emojis/símbolos\n",
    "txt_clean = F.regexp_replace(txt_clean, r\"\\s+\", \" \")                         # espacios\n",
    "\n",
    "df_norm = (\n",
    "    df_std\n",
    "    .withColumn(\"reviewText\", txt_clean)\n",
    "    .withColumn(\"overall\", F.col(\"overall\").cast(\"int\"))\n",
    "    .withColumn(\"helpful_votes\", F.col(\"helpful_votes\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "# 3) Flags y causas simples\n",
    "flag_return = F.expr(\"\"\"IF(reviewText RLIKE '(devolv|devolver|return(ed)?|refund(ed)?|reembolso)', 1, 0)\"\"\")\n",
    "cause = (\n",
    "    F.when(F.col(\"reviewText\").rlike(r\"\\b(no funciona|not work(s|ed)?|does.?t work|dead|defect)\"), \"no_funciona\")\n",
    "     .when(F.col(\"reviewText\").rlike(r\"\\b(no compatible|incompatible|not compatible|does.?t fit)\"), \"no_compatible\")\n",
    "     .when(F.col(\"reviewText\").rlike(r\"\\b(baja calidad|poor qualit(y)?|cheap|frágil|fragil|broken)\"), \"baja_calidad\")\n",
    "     .otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "df_enriched = (\n",
    "    df_norm\n",
    "    .withColumn(\"flag_return\", flag_return.cast(\"int\"))\n",
    "    .withColumn(\"cause\", cause)\n",
    ")\n",
    "\n",
    "# 4) Deduplicación determinística\n",
    "df_key = (\n",
    "    df_enriched\n",
    "    .withColumn(\n",
    "        \"review_id\",\n",
    "        F.md5(F.concat_ws(\"|\",\n",
    "            F.coalesce(F.col(\"asin\"), F.lit(\"\")),\n",
    "            F.coalesce(F.col(\"user_id\"), F.lit(\"\")),\n",
    "            F.date_format(F.col(\"ts_review\"), \"yyyy-MM-dd\"),\n",
    "            F.substring(F.coalesce(F.col(\"reviewText\"), F.lit(\"\")), 1, 256)\n",
    "        ))\n",
    "    )\n",
    "    .withColumn(\"len_text\", F.length(\"reviewText\"))\n",
    ")\n",
    "\n",
    "w = W.partitionBy(\"review_id\").orderBy(F.col(\"len_text\").desc(), F.col(\"ts_review\").desc())\n",
    "df_dedup = (\n",
    "    df_key\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\",\"len_text\")\n",
    ")\n",
    "\n",
    "# 5) Escribir Silver (Parquet particionado por dt)\n",
    "(df_dedup\n",
    " .repartition(\"dt\")\n",
    " .write.mode(\"overwrite\")\n",
    " .partitionBy(\"dt\")\n",
    " .parquet(silver_2023_path)\n",
    ")\n",
    "\n",
    "print(\"Silver Clean escrito en:\", silver_2023_path)\n",
    "\n",
    "# 6) Validaciones rápidas\n",
    "df_s = spark.read.parquet(silver_2023_path)\n",
    "print(\"Filas Silver:\", df_s.count())\n",
    "df_s.groupBy(\"dt\").count().orderBy(\"dt\").show(12, False)\n",
    "df_s.select(F.min(\"ts_review\").alias(\"min_fecha\"),\n",
    "            F.max(\"ts_review\").alias(\"max_fecha\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c560a64a-b820-4bc1-af4f-6e61f5756598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asin', 'reviewText', 'title', 'overall', 'helpful_votes', 'user_id', 'verified_purchase', 'ts_review', 'flag_return', 'cause', 'review_id', 'dt']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver_path = \"hdfs://localhost:9000/datalake/silver/amazon/electronics/reviews_clean_2023/\"\n",
    "df_s = spark.read.parquet(silver_path).cache()\n",
    "print(df_s.columns)  # para que veas los nombres reales\n",
    "\n",
    "# Resolver nombres de columnas según existan\n",
    "cols = set(df_s.columns)\n",
    "text_col    = \"text\" if \"text\" in cols else (\"reviewText\" if \"reviewText\" in cols else None)\n",
    "rating_col  = \"rating\" if \"rating\" in cols else (\"overall\" if \"overall\" in cols else None)\n",
    "helpful_col = \"helpful_votes\" if \"helpful_votes\" in cols else (\"helpful_vote\" if \"helpful_vote\" in cols else None)\n",
    "\n",
    "assert text_col is not None,  \"No encuentro la columna de texto (text/reviewText).\"\n",
    "assert rating_col is not None, \"No encuentro la columna de rating (rating/overall).\"\n",
    "\n",
    "# Renombrar a nombres estándar que usaremos en todo el pipeline\n",
    "df_std = df_s\n",
    "if text_col != \"text\":\n",
    "    df_std = df_std.withColumnRenamed(text_col, \"text\")\n",
    "if rating_col != \"rating\":\n",
    "    df_std = df_std.withColumnRenamed(rating_col, \"rating\")\n",
    "if helpful_col and helpful_col != \"helpful_votes\":\n",
    "    df_std = df_std.withColumnRenamed(helpful_col, \"helpful_votes\")\n",
    "\n",
    "df_std = (\n",
    "    df_std\n",
    "    .withColumn(\"rating_int\", F.col(\"rating\").cast(\"int\"))\n",
    "    .withColumn(\"len_text\",  F.length(F.col(\"text\").cast(\"string\")))\n",
    "    .withColumn(\"len_title\", F.length(F.col(\"title\").cast(\"string\")))\n",
    "    .withColumn(\"vp\", (F.col(\"verified_purchase\")==True).cast(\"int\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87cd76b1-836e-4402-9ea9-4cede2e4b9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:25:16 WARN MemoryStore: Not enough space to cache rdd_98_3 in memory! (computed 60.1 MiB so far)\n",
      "25/11/17 20:25:16 WARN BlockManager: Persisting block rdd_98_3 to disk instead.\n",
      "25/11/17 20:25:17 WARN MemoryStore: Not enough space to cache rdd_98_2 in memory! (computed 120.3 MiB so far)\n",
      "25/11/17 20:25:17 WARN BlockManager: Persisting block rdd_98_2 to disk instead.\n",
      "25/11/17 20:25:19 WARN MemoryStore: Not enough space to cache rdd_98_2 in memory! (computed 120.3 MiB so far)\n",
      "25/11/17 20:25:22 WARN MemoryStore: Not enough space to cache rdd_98_3 in memory! (computed 60.1 MiB so far)\n",
      "25/11/17 20:25:24 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 49.4 MiB so far)\n",
      "25/11/17 20:25:24 WARN BlockManager: Persisting block rdd_104_0 to disk instead.\n",
      "25/11/17 20:25:25 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 94.8 MiB so far)\n",
      "25/11/17 20:25:25 WARN BlockManager: Persisting block rdd_104_1 to disk instead.\n",
      "25/11/17 20:25:28 WARN MemoryStore: Not enough space to cache rdd_98_2 in memory! (computed 120.3 MiB so far)\n",
      "25/11/17 20:25:29 WARN MemoryStore: Not enough space to cache rdd_98_3 in memory! (computed 114.0 MiB so far)\n",
      "25/11/17 20:25:34 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 95.5 MiB so far)\n",
      "25/11/17 20:25:34 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 94.8 MiB so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|label_bin|  count|\n",
      "+---------+-------+\n",
      "|        1| 389777|\n",
      "|        0|1372263|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_bin = (\n",
    "    df_std\n",
    "    .filter(F.col(\"rating_int\").isin(1,2,3,4,5))\n",
    "    .withColumn(\n",
    "        \"label_bin\",\n",
    "        F.when(F.col(\"rating_int\").isin(1,2), F.lit(1))\n",
    "         .when(F.col(\"rating_int\").isin(4,5), F.lit(0))\n",
    "    )\n",
    "    .filter(F.col(\"label_bin\").isNotNull())\n",
    "    .select(\"asin\",\"dt\",\"ts_review\",\"text\",\"title\",\"rating_int\",\"label_bin\",\n",
    "            \"len_text\",\"len_title\",\"vp\",\n",
    "            *([\"helpful_votes\"] if \"helpful_votes\" in df_std.columns else []))\n",
    ").cache()\n",
    "\n",
    "df_bin.groupBy(\"label_bin\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bbdad8-d7b5-4dea-ba3d-a494c79539a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:25:42 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 95.5 MiB so far)\n",
      "25/11/17 20:25:42 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 94.8 MiB so far)\n",
      "25/11/17 20:26:17 WARN MemoryStore: Not enough space to cache rdd_104_0 in memory! (computed 95.5 MiB so far)\n",
      "25/11/17 20:26:50 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 48.9 MiB so far)\n",
      "25/11/17 20:27:24 WARN MemoryStore: Not enough space to cache rdd_104_3 in memory! (computed 90.4 MiB so far)\n",
      "25/11/17 20:28:08 WARN DAGScheduler: Broadcasting large task binary with size 1534.5 KiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, CountVectorizer, IDF, VectorAssembler\n",
    "\n",
    "tok = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=r\"[^a-zA-Z0-9]+\", minTokenLength=2, toLowercase=True)\n",
    "rm  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_sw\")\n",
    "ng2 = NGram(n=2, inputCol=\"tokens_sw\", outputCol=\"bi\")\n",
    "\n",
    "# Unigramas\n",
    "cv_u  = CountVectorizer(inputCol=\"tokens_sw\", outputCol=\"tf_u\", vocabSize=100000, minDF=20)\n",
    "idf_u = IDF(inputCol=\"tf_u\", outputCol=\"tfidf_u\")\n",
    "\n",
    "# Bigramas\n",
    "cv_b  = CountVectorizer(inputCol=\"bi\", outputCol=\"tf_b\", vocabSize=100000, minDF=20)\n",
    "idf_b = IDF(inputCol=\"tf_b\", outputCol=\"tfidf_b\")\n",
    "\n",
    "num_feats = [c for c in [\"len_text\",\"len_title\",\"vp\",\"helpful_votes\"] if c in df_bin.columns]\n",
    "assembler = VectorAssembler(inputCols=[\"tfidf_u\",\"tfidf_b\"] + num_feats, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tok, rm, ng2, cv_u, idf_u, cv_b, idf_b, assembler])\n",
    "\n",
    "train, test = df_bin.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "gold_train = model.transform(train).select(\"asin\",\"dt\",\"ts_review\",\"rating_int\",\"label_bin\",\"features\")\n",
    "gold_test  = model.transform(test ).select(\"asin\",\"dt\",\"ts_review\",\"rating_int\",\"label_bin\",\"features\")\n",
    "\n",
    "# (opcional) guarda los vocabularios para interpretabilidad:\n",
    "from pyspark.ml.feature import CountVectorizerModel\n",
    "cv_u_model = next(s for s in model.stages if isinstance(s, CountVectorizerModel) and s.getOutputCol()==\"tf_u\")\n",
    "cv_b_model = next(s for s in model.stages if isinstance(s, CountVectorizerModel) and s.getOutputCol()==\"tf_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5006520e-5d28-4f77-810f-b91d68b04abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:28:57 WARN DAGScheduler: Broadcasting large task binary with size 10.2 MiB\n",
      "25/11/17 20:28:58 WARN MemoryStore: Not enough space to cache rdd_104_1 in memory! (computed 48.9 MiB so far)\n",
      "25/11/17 20:30:00 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "25/11/17 20:30:18 WARN DAGScheduler: Broadcasting large task binary with size 10.2 MiB\n",
      "25/11/17 20:30:37 WARN DAGScheduler: Broadcasting large task binary with size 14.8 MiB\n",
      "[Stage 60:======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold features -> hdfs://localhost:9000/datalake/gold/amazon/electronics/reviews_gold_features_2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "gold_base = \"hdfs://localhost:9000/datalake/gold/amazon/electronics\"\n",
    "gold_feat_path = f\"{gold_base}/reviews_gold_features_2023\"\n",
    "\n",
    "# particiona por dt para consultas cómodas\n",
    "(gold_train\n",
    " .repartition(\"dt\")\n",
    " .write.mode(\"overwrite\")\n",
    " .partitionBy(\"dt\")\n",
    " .parquet(f\"{gold_feat_path}/train\"))\n",
    "\n",
    "(gold_test\n",
    " .repartition(\"dt\")\n",
    " .write.mode(\"overwrite\")\n",
    " .partitionBy(\"dt\")\n",
    " .parquet(f\"{gold_feat_path}/test\"))\n",
    "\n",
    "print(\"Gold features ->\", gold_feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2a8503-8816-4222-863c-256fb9c31531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:31:01 WARN Utils: Your hostname, vbox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/17 20:31:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/17 20:31:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Read_Gold_2023\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", str(128*1024*1024))\n",
    "    # opcional si tu VM aguanta:\n",
    "    # .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7519623b-2d59-461c-83c0-60fb6827ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      " |-- rating_int: integer (nullable = true)\n",
      " |-- label_bin: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|asin      |rating_int|label_bin|\n",
      "+----------+----------+---------+\n",
      "|0062970704|5         |0        |\n",
      "|B09Y1ZST8V|5         |0        |\n",
      "|0511189877|1         |1        |\n",
      "|B09Y1ZST8V|5         |0        |\n",
      "|0594450233|5         |0        |\n",
      "+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_base = \"hdfs://localhost:9000/datalake/gold/amazon/electronics\"\n",
    "gold_feat_path = f\"{gold_base}/reviews_gold_features_2023\"\n",
    "\n",
    "gold_train_path = f\"{gold_feat_path}/train\"\n",
    "gold_test_path  = f\"{gold_feat_path}/test\"\n",
    "\n",
    "df_train = spark.read.parquet(gold_train_path) \\\n",
    "    .select(\"asin\",\"dt\",\"rating_int\",\"label_bin\",\"features\")\n",
    "df_test  = spark.read.parquet(gold_test_path) \\\n",
    "    .select(\"asin\",\"dt\",\"rating_int\",\"label_bin\",\"features\")\n",
    "\n",
    "df_train.printSchema()\n",
    "df_train.limit(5).select(\"asin\",\"rating_int\",\"label_bin\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a79bce-a7fb-4074-89c4-0ed5c9f3679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_small: 423561 test_small: 106088\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# ============================\n",
    "# 1) (Opcional) Muestrear para evitar OOM\n",
    "# ============================\n",
    "# Si esto va bien, luego puedes subir el 0.3 a 0.5 o incluso quitar el sample.\n",
    "train_small = df_train.sample(False, 0.3, seed=42)\n",
    "test_small  = df_test.sample(False, 0.3, seed=42)\n",
    "\n",
    "print(\"train_small:\", train_small.count(), \"test_small:\", test_small.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f727f32-9289-4e55-90a6-3730c335ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:31:51 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:31:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/11/17 20:31:59 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:32:44 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:33:37 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:34:32 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:35:27 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:36:22 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:37:19 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:38:15 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:39:08 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:40:01 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:40:55 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:41:49 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:42:27 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:43:08 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:44:05 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:44:49 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:45:35 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:46:25 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:47:04 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:47:50 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:48:39 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n",
      "25/11/17 20:49:29 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "25/11/17 20:49:35 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "25/11/17 20:49:39 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.961  |  Accuracy: 0.903  |  F1: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:49:43 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "[Stage 46:======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|label_bin|prediction|count|\n",
      "+---------+----------+-----+\n",
      "|        0|       0.0|80796|\n",
      "|        0|       1.0| 1555|\n",
      "|        1|       0.0| 8697|\n",
      "|        1|       1.0|15040|\n",
      "+---------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:49:47 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 2) Modelo: Regresión Logística binaria\n",
    "# ============================\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label_bin\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_small)\n",
    "\n",
    "# ============================\n",
    "# 3) Predicciones y métricas\n",
    "# ============================\n",
    "pred_test = lr_model.transform(test_small)\n",
    "\n",
    "e_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label_bin\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "e_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_bin\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "e_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_bin\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "auc = e_auc.evaluate(pred_test)\n",
    "acc = e_acc.evaluate(pred_test)\n",
    "f1  = e_f1.evaluate(pred_test)\n",
    "\n",
    "print(f\"AUC: {auc:.3f}  |  Accuracy: {acc:.3f}  |  F1: {f1:.3f}\")\n",
    "\n",
    "# Una tablita rápida de matriz de confusión\n",
    "pred_test.groupBy(\"label_bin\", \"prediction\").count().orderBy(\"label_bin\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb96499-6b5a-45a9-8aa4-83606420341c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: hdfs://localhost:9000/datalake/gold/models/lr_reviews_2023_v1\n"
     ]
    }
   ],
   "source": [
    "model_path_hdfs = \"hdfs://localhost:9000/datalake/gold/models/lr_reviews_2023_v1\"\n",
    "\n",
    "lr_model.write().overwrite().save(model_path_hdfs)\n",
    "print(\"Modelo guardado en:\", model_path_hdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782b2e45-097b-44f7-831d-684f90c63b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:51:37 WARN DAGScheduler: Broadcasting large task binary with size 7.6 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+-----------------------------------------+----------+\n",
      "|asin      |rating_int|label_bin|probability                              |prediction|\n",
      "+----------+----------+---------+-----------------------------------------+----------+\n",
      "|0547152469|4         |0        |[0.9425286205305041,0.057471379469495876]|0.0       |\n",
      "|0823430499|4         |0        |[0.9790312279554321,0.02096877204456793] |0.0       |\n",
      "|0972683275|5         |0        |[0.966049698360309,0.03395030163969104]  |0.0       |\n",
      "|1426320965|5         |0        |[0.8587313589781541,0.1412686410218459]  |0.0       |\n",
      "|1449410243|5         |0        |[0.9288930662075883,0.07110693379241173] |0.0       |\n",
      "|1449410243|1         |1        |[0.6865070801728896,0.31349291982711036] |0.0       |\n",
      "|1529427975|5         |0        |[0.8495914618664331,0.15040853813356692] |0.0       |\n",
      "|1593278551|5         |0        |[0.9072620092994566,0.09273799070054345] |0.0       |\n",
      "|4757567812|2         |1        |[0.8968952044855124,0.10310479551448759] |0.0       |\n",
      "|8537641820|5         |0        |[0.7958774354014163,0.2041225645985837]  |0.0       |\n",
      "+----------+----------+---------+-----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "model_path_hdfs = \"hdfs://localhost:9000/datalake/gold/models/lr_reviews_2023_v1\"\n",
    "\n",
    "lr_loaded = LogisticRegressionModel.load(model_path_hdfs)\n",
    "\n",
    "# Prueba rápida sobre un pequeño sample de gold_test\n",
    "pred_sample = lr_loaded.transform(df_test.limit(10))\n",
    "pred_sample.select(\"asin\",\"rating_int\",\"label_bin\",\"probability\",\"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e6dc27-4ddb-473a-9fd4-97f9dc4caf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold rows: 1762040\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- dt: string (nullable = true)\n",
      " |-- rating_int: integer (nullable = true)\n",
      " |-- label_bin: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:52:22 WARN DAGScheduler: Broadcasting large task binary with size 13.3 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|asin      |dt     |rating_int|label_bin|features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "+----------+-------+----------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0062970704|2023-01|5         |0        |(122457,[29,96,104,107,133,357,479,496,583,1039,1082,1206,1257,1322,13701,20616,33600,41976,45862,122453,122454,122455],[2.8541950971254404,3.5384782865388154,3.5438878729619656,3.5576034751519385,3.687742315355486,4.434745753031512,4.73404012166927,4.756810110193251,4.996517359515511,5.700635114238067,5.653806587552175,5.883550822897737,5.860636563374861,6.061196624069924,10.031792812689051,10.940051372865943,8.738392198461858,9.168494610955408,9.314740111275553,199.0,20.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|B09Y1ZST8V|2023-01|5         |0        |(122457,[1,6,7,14,19,39,92,93,130,172,183,255,295,410,441,475,765,896,906,1096,2882,4292,22472,22703,22888,23697,25697,34001,39805,40296,45391,52633,64250,74967,78231,81459,106572,122453,122454,122455,122456],[1.5428829469579444,2.108812697847199,2.1704945260223862,2.486224057071525,2.6640536933716636,9.823568688042574,3.475816647701794,3.563768553521437,3.70379842693225,4.0866255717625615,4.063580383923572,4.146316218179662,4.3938400149340415,4.631443043156561,4.76551555007346,4.825661478920662,5.273347906451162,5.4780857147895725,5.568297688244724,5.6858949021036755,7.105341470540467,7.773732798736418,4.841168428990691,6.180616227866422,6.53138280724564,7.2081124292915595,7.847192388581229,8.811819667016675,9.07752283274968,9.096332164707176,9.306896933814526,9.524198209504508,9.789479345267122,9.984539927838506,10.031792812689051,10.081389753828423,10.39772708204058,270.0,23.0,1.0,1.0])|\n",
      "|0511189877|2023-01|1         |1        |(122457,[11,30,231,319,328,381,424,455,675,901,2564,6949,34562,38280,50677,53374,82338,99549,122453,122454,122455],[2.345319529385115,2.9128229145483666,4.044692614220357,4.361911889708532,4.38668793180797,4.502811854246298,4.63020582455691,4.684915723267683,5.107582557248419,5.425655352725818,6.916844838477181,8.695095392708533,8.792951182712292,9.011432720920691,9.467579315505,9.543806680892883,10.098484187187724,10.308779596024085,132.0,16.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|B09Y1ZST8V|2023-01|5         |0        |(122457,[1,71,88,230,696,724,760,22555,23034,51583,122453,122454,122455],[1.5428829469579444,3.330526196032975,3.418820023381677,4.02480422812551,5.253754212550761,5.171855384885322,5.230817119531497,5.7166730929824,6.752823816497129,9.495488103622076,79.0,14.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|0594450233|2023-01|5         |0        |(122457,[1,3,195,356,2446,29508,34724,38693,55167,60139,122453,122454,122455],[3.0857658939158887,1.8891429353575357,4.140594253528975,4.522339178467268,6.867952419591162,8.393736094949299,8.807069064258076,9.023128760683882,9.584216219230761,9.704579901480635,47.0,28.0,1.0])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+----------+-------+----------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "# --- Rutas en HDFS ---\n",
    "gold_base = \"hdfs://localhost:9000/datalake/gold/amazon/electronics\"\n",
    "gold_feat_path = f\"{gold_base}/reviews_gold_features_2023\"\n",
    "\n",
    "model_path_hdfs = \"hdfs://localhost:9000/datalake/gold/models/lr_reviews_2023_v1\"\n",
    "\n",
    "# 1) Cargar modelo entrenado\n",
    "lr_model = LogisticRegressionModel.load(model_path_hdfs)\n",
    "\n",
    "# 2) Cargar Gold (train + test) y unirlos\n",
    "df_train = (\n",
    "    spark.read.parquet(f\"{gold_feat_path}/train\")\n",
    "         .select(\"asin\", \"dt\", \"rating_int\", \"label_bin\", \"features\")\n",
    ")\n",
    "\n",
    "df_test = (\n",
    "    spark.read.parquet(f\"{gold_feat_path}/test\")\n",
    "         .select(\"asin\", \"dt\", \"rating_int\", \"label_bin\", \"features\")\n",
    ")\n",
    "\n",
    "df_gold = df_train.unionByName(df_test)\n",
    "\n",
    "print(\"Gold rows:\", df_gold.count())\n",
    "df_gold.printSchema()\n",
    "df_gold.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba60c969-20b0-4420-91c7-31b8ad420145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:59:22 WARN DAGScheduler: Broadcasting large task binary with size 14.3 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------------------+----------+\n",
      "|asin      |dt     |label_bin|prob_neg            |prediction|\n",
      "+----------+-------+---------+--------------------+----------+\n",
      "|0062970704|2023-01|0        |0.12710706265746718 |0.0       |\n",
      "|B09Y1ZST8V|2023-01|0        |0.017681150651766786|0.0       |\n",
      "|0511189877|2023-01|1        |0.5003681915851966  |1.0       |\n",
      "|B09Y1ZST8V|2023-01|0        |0.03836144070524661 |0.0       |\n",
      "|0594450233|2023-01|0        |0.040787941720065835|0.0       |\n",
      "+----------+-------+---------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:59:24 WARN DAGScheduler: Broadcasting large task binary with size 14.3 MiB\n",
      "25/11/17 20:59:38 WARN DAGScheduler: Broadcasting large task binary with size 14.3 MiB\n",
      "25/11/17 20:59:41 WARN DAGScheduler: Broadcasting large task binary with size 14.3 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+-----+------------------+-------+---------+\n",
      "|asin      |dt     |n_reviews|n_neg|avg_prob_neg      |pct_neg|risk_rank|\n",
      "+----------+-------+---------+-----+------------------+-------+---------+\n",
      "|B09FH46J9P|2022-12|1        |1    |0.9999999751244969|1.0    |1        |\n",
      "|B0BLGW32NR|2022-12|1        |1    |0.9998835808584299|1.0    |2        |\n",
      "|B0BL2B4PNG|2022-12|1        |1    |0.9998786730860699|1.0    |3        |\n",
      "|B097CNBDX2|2022-12|1        |1    |0.9998529800145185|1.0    |4        |\n",
      "|B07QKXM2D3|2022-12|1        |1    |0.9997692749529943|1.0    |5        |\n",
      "|B07QZF1XHJ|2022-12|1        |1    |0.9996100703065052|1.0    |6        |\n",
      "|B09V2JG9G1|2022-12|1        |1    |0.9994693387092841|1.0    |7        |\n",
      "|B0BFDXCRP9|2022-12|1        |1    |0.9984915743536167|1.0    |8        |\n",
      "|B0B3XF97HV|2022-12|1        |1    |0.9983995069234456|1.0    |9        |\n",
      "|B0836GXKKB|2022-12|1        |1    |0.9979716660263701|1.0    |10       |\n",
      "+----------+-------+---------+-----+------------------+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/17 20:59:43 WARN DAGScheduler: Broadcasting large task binary with size 14.3 MiB\n",
      "25/11/17 20:59:54 WARN DAGScheduler: Broadcasting large task binary with size 14.3 MiB\n",
      "25/11/17 20:59:55 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n",
      "[Stage 78:=============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KPI1 guardado en: hdfs://localhost:9000/datalake/gold/results/ranking_riesgo_mensual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# 3) Scoring sobre TODA la Gold\n",
    "pred_all = lr_model.transform(df_gold)\n",
    "\n",
    "# Pasar de VectorUDT -> array<double>\n",
    "pred_all = pred_all.withColumn(\"prob_array\", vector_to_array(\"probability\"))\n",
    "\n",
    "# probability es [p_clase0, p_clase1]; usamos p_clase1 = prob de \"riesgo\" (1–2 estrellas)\n",
    "pred_all = pred_all.withColumn(\"prob_neg\", F.col(\"prob_array\")[1].cast(\"double\")) \\\n",
    "                   .drop(\"prob_array\")\n",
    "\n",
    "pred_all.select(\"asin\", \"dt\", \"label_bin\", \"prob_neg\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# 4) KPI 1: riesgo mensual por ASIN\n",
    "#   - n_reviews: total\n",
    "#   - n_neg: # de reseñas negativas (label_bin=1)\n",
    "#   - pct_neg: porcentaje de negativas\n",
    "#   - avg_prob_neg: promedio de probabilidad de clase 1 en las reseñas negativas\n",
    "\n",
    "kpi1 = (\n",
    "    pred_all\n",
    "    .groupBy(\"asin\", \"dt\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"n_reviews\"),\n",
    "        F.sum(F.when(F.col(\"label_bin\") == 1, 1).otherwise(0)).alias(\"n_neg\"),\n",
    "        F.avg(F.when(F.col(\"label_bin\") == 1, F.col(\"prob_neg\")).otherwise(None)).alias(\"avg_prob_neg\")\n",
    "    )\n",
    "    .withColumn(\"pct_neg\", F.col(\"n_neg\") / F.col(\"n_reviews\"))\n",
    ")\n",
    "\n",
    "# (Opcional) ranking dentro de cada mes, por riesgo\n",
    "from pyspark.sql.window import Window\n",
    "w = Window.partitionBy(\"dt\").orderBy(F.col(\"avg_prob_neg\").desc())\n",
    "kpi1 = kpi1.withColumn(\"risk_rank\", F.row_number().over(w))\n",
    "\n",
    "kpi1.show(10, truncate=False)\n",
    "\n",
    "# 5) Guardar KPI1 en HDFS (Zona Gold / results)\n",
    "risk_path = \"hdfs://localhost:9000/datalake/gold/results/ranking_riesgo_mensual\"\n",
    "\n",
    "(\n",
    "  kpi1\n",
    "  .repartition(\"dt\")\n",
    "  .write.mode(\"overwrite\")\n",
    "  .partitionBy(\"dt\")\n",
    "  .parquet(risk_path)\n",
    ")\n",
    "\n",
    "print(\"KPI1 guardado en:\", risk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b6dcf2-8350-49bb-88ff-24a502a6046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+-------------+------------+\n",
      "|asin      |dt     |no_funciona|no_compatible|baja_calidad|\n",
      "+----------+-------+-----------+-------------+------------+\n",
      "|B0B3SB76HB|2023-01|0          |0            |1           |\n",
      "|B07QGVMCJG|2023-01|0          |1            |4           |\n",
      "|B098FKXT8L|2023-01|7          |0            |4           |\n",
      "|B0BGKD69FH|2023-01|0          |0            |1           |\n",
      "|B09SNQ6FQ2|2023-01|1          |0            |1           |\n",
      "|B0BGJ2NNJ2|2023-01|2          |0            |5           |\n",
      "|B08X4Y6CGH|2023-01|1          |0            |1           |\n",
      "|B09WTNWC29|2023-01|3          |0            |0           |\n",
      "|B09KLB9JMN|2023-01|1          |0            |0           |\n",
      "|B09JFTS3PF|2023-01|3          |0            |2           |\n",
      "+----------+-------+-----------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KPI2 guardado en: hdfs://localhost:9000/datalake/gold/results/mapa_causas_mensual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 6) Leer Silver 2023 para el mapa de causas\n",
    "silver_path = \"hdfs://localhost:9000/datalake/silver/amazon/electronics/reviews_clean_2023/\"\n",
    "\n",
    "df_silver = spark.read.parquet(silver_path).select(\"asin\", \"dt\", \"cause\")\n",
    "\n",
    "# Nos quedamos solo con filas que tienen causa\n",
    "df_ca = df_silver.filter(F.col(\"cause\").isNotNull())\n",
    "\n",
    "# 7) KPI 2: conteo de causas por ASIN y mes\n",
    "# Puedes dejarlo \"largo\" (asin, dt, cause, count)...\n",
    "kpi2_long = (\n",
    "    df_ca.groupBy(\"asin\", \"dt\", \"cause\")\n",
    "         .count()\n",
    ")\n",
    "\n",
    "# ...o pivotear para tener columnas por causa (no_funciona, no_compatible, baja_calidad)\n",
    "kpi2 = (\n",
    "    kpi2_long\n",
    "    .groupBy(\"asin\", \"dt\")\n",
    "    .pivot(\"cause\", [\"no_funciona\", \"no_compatible\", \"baja_calidad\"])\n",
    "    .sum(\"count\")\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "kpi2.show(10, truncate=False)\n",
    "\n",
    "causas_path = \"hdfs://localhost:9000/datalake/gold/results/mapa_causas_mensual\"\n",
    "\n",
    "(\n",
    "  kpi2\n",
    "  .repartition(\"dt\")\n",
    "  .write.mode(\"overwrite\")\n",
    "  .partitionBy(\"dt\")\n",
    "  .parquet(causas_path)\n",
    ")\n",
    "\n",
    "print(\"KPI2 guardado en:\", causas_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0beb7b0-6eda-4937-b33a-a25509499755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

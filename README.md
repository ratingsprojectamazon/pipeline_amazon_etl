# Pipeline ETL Amazon Electronics Reviews 2023

Este proyecto implementa un pipeline completo de ETL (Extract, Transform, Load) para procesar reseÃ±as de productos electrÃ³nicos de Amazon del aÃ±o 2023, utilizando Apache Spark y PySpark. El pipeline incluye desde la extracciÃ³n de datos en formato JSON hasta la creaciÃ³n de modelos de machine learning para clasificaciÃ³n de sentimientos.

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n del Proyecto](#descripciÃ³n-del-proyecto)
- [Arquitectura del Pipeline](#arquitectura-del-pipeline)
- [Estructura de Datos](#estructura-de-datos)
- [InstalaciÃ³n y ConfiguraciÃ³n](#instalaciÃ³n-y-configuraciÃ³n)
- [Uso del Pipeline](#uso-del-pipeline)
- [Capas del Data Lake](#capas-del-data-lake)
- [Modelo de Machine Learning](#modelo-de-machine-learning)
- [Resultados y MÃ©tricas](#resultados-y-mÃ©tricas)

## ğŸ¯ DescripciÃ³n del Proyecto

Este pipeline ETL procesa aproximadamente **43.8 millones** de reseÃ±as de productos electrÃ³nicos de Amazon, filtrando especÃ­ficamente las del aÃ±o 2023 (resultando en **1.9 millones** de registros). El objetivo principal es:

- Limpiar y estructurar los datos de reseÃ±as
- Implementar detecciÃ³n de productos devueltos y sus causas
- Crear features para anÃ¡lisis de sentimientos
- Entrenar un modelo de clasificaciÃ³n binaria para predecir reseÃ±as negativas vs positivas

## ğŸ—ï¸ Arquitectura del Pipeline

El pipeline sigue una arquitectura de Data Lake con mÃºltiples capas:

### TecnologÃ­as Utilizadas

- **Apache Spark**: Motor de procesamiento distribuido
- **PySpark**: API de Python para Spark
- **HDFS**: Sistema de archivos distribuido
- **Parquet**: Formato de almacenamiento columnar
- **Spark ML**: Biblioteca de machine learning

## ğŸ“Š Estructura de Datos

### Schema Original
- asin: string (ID del producto)
- helpful_vote: long (votos Ãºtiles)
- images: array (imÃ¡genes del review)
- parent_asin: string (ASIN padre)
- rating: double (calificaciÃ³n 1-5)
- text: string (texto de la reseÃ±a)
- timestamp: long (timestamp Unix)
- title: string (tÃ­tulo de la reseÃ±a)
- user_id: string (ID del usuario)
- verified_purchase: boolean (compra verificada)


## âš™ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- Apache Spark 3.x
- Hadoop/HDFS configurado
- Python 3.7+
- PySpark
- Jupyter Notebook

### ConfiguraciÃ³n de Spark

```python
spark = (
    SparkSession.builder
    .appName("ETL_Amazon_Electronics_2023")
    .master("local[*]")
    .config("spark.sql.shuffle.partitions", "8")
    .config("spark.sql.files.maxPartitionBytes", str(128 * 1024 * 1024))
    .getOrCreate()
)
```
## Estructura de Directorios HDFS
```code
hdfs://localhost:9000/datalake/
â”œâ”€â”€ landing/
â”‚   â””â”€â”€ Electronics.jsonl
â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ amazon/electronics/reviews/curated_2023/
â”œâ”€â”€ silver/
â”‚   â””â”€â”€ amazon/electronics/reviews_clean_2023/
â””â”€â”€ gold/
    â”œâ”€â”€ amazon/electronics/reviews_gold_features_2023/
    â””â”€â”€ models/lr_reviews_2023_v1/
```
## ğŸš€ Uso del Pipeline

1. EjecuciÃ³n del Pipeline Completo
El pipeline se ejecuta a travÃ©s del notebook Jupyter ETL 2023 BIG DATA_2.ipynb:
```bash
jupyter notebook "ETL 2023 BIG DATA_2.ipynb"
```
